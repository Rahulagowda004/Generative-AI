{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM,AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"openAI-community/gpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openAI-community/gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def generate(model, tokenizer, input_ids, max_length=50, do_sample=False, top_k=None):\n",
    "    generated = input_ids\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            outputs = model(input_ids=input_ids)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            if do_sample:\n",
    "                if top_k is not None:\n",
    "                    logits = top_k_logits(logits, top_k)\n",
    "                next_token = torch.multinomial(torch.nn.functional.softmax(logits[:, -1, :], dim=-1), num_samples=1)\n",
    "            else:\n",
    "                next_token = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n",
    "\n",
    "            generated = torch.cat((generated, next_token), dim=1)\n",
    "\n",
    "            if next_token.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "\n",
    "            input_ids = generated\n",
    "\n",
    "    return generated\n",
    "\n",
    "def top_k_logits(logits, k):\n",
    "    values, _ = torch.topk(logits, k)\n",
    "    min_values = values[:, -1].unsqueeze(-1)\n",
    "    return torch.where(logits < min_values, torch.full_like(logits, -float('Inf')), logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hey?\"\n",
      "\n",
      "On that note.. remember remember Kerry being the first country that picked his cable news show out of the 1000st Days of Helping Kids In Colorado rodeo where the Johnny Varela juggle students, scientists and audience members became the\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "prompts = \"hey \"\n",
    "encoding = tokenizer(prompts, return_tensors=\"pt\")\n",
    "input_ids = encoding['input_ids']\n",
    "\n",
    "model.eval()\n",
    "\n",
    "generated_ids = generate(model=model, tokenizer=tokenizer, input_ids=input_ids,do_sample=True)\n",
    "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(generated_text)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
